{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a5469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 1: Montowanie Google Drive i konfiguracja Kaggle API\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Wykrycie środowiska (Colab vs lokalnie)\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    _IN_COLAB = True\n",
    "except Exception:\n",
    "    _IN_COLAB = False\n",
    "\n",
    "# Ustalenie katalogu roboczego (w Colab: Google Drive, lokalnie: repo)\n",
    "if _IN_COLAB:\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_WORKING = \"/content/drive/MyDrive/mass_detection_training\"\n",
    "    REPO_ROOT = Path(\"/content\")\n",
    "else:\n",
    "    cwd = Path.cwd().resolve()\n",
    "    candidates = [cwd, cwd.parent]\n",
    "    REPO_ROOT = next((p for p in candidates if (p / \"requirements.txt\").exists()), cwd)\n",
    "    DRIVE_WORKING = str(REPO_ROOT / \"analysis\" / \"working\")\n",
    "    print(\"Wykryto środowisko lokalne (bez google.colab)\")\n",
    "\n",
    "os.makedirs(DRIVE_WORKING, exist_ok=True)\n",
    "os.makedirs(f\"{DRIVE_WORKING}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{DRIVE_WORKING}/logs\", exist_ok=True)\n",
    "os.makedirs(f\"{DRIVE_WORKING}/splits\", exist_ok=True)\n",
    "\n",
    "print(\"Katalog roboczy:\", DRIVE_WORKING)\n",
    "\n",
    "# Konfiguracja Kaggle API\n",
    "# Preferuj token z repo (kaggle.json w katalogu projektu), a w Colabie wspieraj też DRIVE_WORKING/kaggle.json.\n",
    "kaggle_json_candidates = [\n",
    "    Path(DRIVE_WORKING) / \"kaggle.json\",\n",
    "    REPO_ROOT / \"kaggle.json\",\n",
    "]\n",
    "\n",
    "kaggle_json_src = next((p for p in kaggle_json_candidates if p.exists()), None)\n",
    "if kaggle_json_src is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Nie znaleziono pliku kaggle.json. Umieść go w jednym z miejsc:\\n\"\n",
    "        f\"- {kaggle_json_candidates[0]}\\n\"\n",
    "        f\"- {kaggle_json_candidates[1]}\"\n",
    "    )\n",
    "\n",
    "kaggle_dir = Path(os.path.expanduser(\"~/.kaggle\"))\n",
    "kaggle_dir.mkdir(parents=True, exist_ok=True)\n",
    "kaggle_json_dst = kaggle_dir / \"kaggle.json\"\n",
    "\n",
    "shutil.copyfile(kaggle_json_src, kaggle_json_dst)\n",
    "os.chmod(kaggle_json_dst, 0o600)\n",
    "\n",
    "print(\"Skonfigurowano Kaggle API:\", kaggle_json_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae05d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 2: Pobieranie zbioru NIH Chest X-ray z Kaggle (45 GB)\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Jeśli masz już dane w repo (ClearScanV2/data), pomiń pobieranie z Kaggle.\n",
    "local_metadata = REPO_ROOT / \"data\" / \"metadata\" / \"Data_Entry_2017_v2020.csv\"\n",
    "local_images_dir = REPO_ROOT / \"data\" / \"images\"\n",
    "\n",
    "if local_metadata.exists() and local_images_dir.exists():\n",
    "    data_path = str(REPO_ROOT / \"data\")\n",
    "    print(\"Wykryto lokalny zbiór danych w repo – pomijam pobieranie z Kaggle.\")\n",
    "    print(f\"Ścieżka do zbioru danych: {data_path}\")\n",
    "else:\n",
    "    import subprocess\n",
    "\n",
    "    print(\"Pobieranie zbioru NIH Chest X-ray z Kaggle...\")\n",
    "\n",
    "    # Instalacja kagglehub\n",
    "    try:\n",
    "        import kagglehub\n",
    "    except ImportError:\n",
    "        print(\"Instaluję kagglehub...\")\n",
    "        subprocess.run([\"pip\", \"install\", \"-q\", \"kagglehub\"], check=True)\n",
    "        import kagglehub\n",
    "\n",
    "    # Pobranie zbioru danych za pomocą kagglehub (zalecany sposób w Colabie)\n",
    "    data_path = kagglehub.dataset_download('nih-chest-xrays/data')\n",
    "    print(\"Zbiór danych został pomyślnie pobrany!\")\n",
    "    print(f\"Ścieżka do zbioru danych: {data_path}\")\n",
    "\n",
    "    # Sprawdzenie, czy istnieją pliki metadanych\n",
    "    metadata_file = None\n",
    "    for root, dirs, files in os.walk(data_path):\n",
    "        if \"Data_Entry_2017.csv\" in files:\n",
    "            metadata_file = os.path.join(root, \"Data_Entry_2017.csv\")\n",
    "            print(f\"Znaleziono plik metadanych: {metadata_file}\")\n",
    "            break\n",
    "\n",
    "    if metadata_file is None:\n",
    "        raise FileNotFoundError(\"Nie znaleziono pliku Data_Entry_2017.csv w pobranym zbiorze danych\")\n",
    "\n",
    "print(\"\\nImport źródła danych zakończony.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac470e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 3: Konfiguracja środowiska i instalacja pakietów\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"OpenCV: {cv2.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Dostępność CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Wykryto GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Pamięć GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"UWAGA: Nie wykryto GPU! (To OK lokalnie, ale trening może być wolny)\")\n",
    "\n",
    "# Instalacja pakietów (opcjonalnie)\n",
    "try:\n",
    "    import albumentations as _A\n",
    "    print(f\"Albumentations: {_A.__version__}\")\n",
    "except Exception:\n",
    "    print(\"Instaluję albumentations/qudida/pyyaml...\")\n",
    "    import subprocess, sys\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"--no-deps\", \"albumentations==1.4.0\"], check=True)\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"qudida\", \"pyyaml\"], check=True)\n",
    "\n",
    "print(\"\\nKonfiguracja środowiska zakończona.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0929aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 4: Importy\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import gc\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import torchvision.models as models\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from sklearn.metrics import (roc_auc_score, accuracy_score, confusion_matrix,\n",
    "                            classification_report, roc_curve, precision_recall_curve)\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Wszystkie moduły zostały poprawnie zaimportowane\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61587f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 5: Ścieżki i konfiguracja\n",
    "\n",
    "# Ścieżki (w Colabie – Google Drive; lokalnie – katalog w repo)\n",
    "COLAB_WORKING = Path(DRIVE_WORKING)\n",
    "\n",
    "# Wykrywanie lokalizacji zbioru danych\n",
    "print(\"Wyszukiwanie lokalizacji zbioru danych...\")\n",
    "\n",
    "local_metadata = REPO_ROOT / \"data\" / \"metadata\" / \"Data_Entry_2017_v2020.csv\"\n",
    "local_images = REPO_ROOT / \"data\" / \"images\"\n",
    "\n",
    "if local_metadata.exists() and local_images.exists():\n",
    "    COLAB_DATA = local_images\n",
    "    METADATA_PATH = local_metadata\n",
    "    print(f\"Znaleziono lokalny zbiór danych w: {COLAB_DATA}\")\n",
    "else:\n",
    "    possible_paths = list(Path.home().glob(\"**/**/Data_Entry_2017.csv\"))\n",
    "    if not possible_paths:\n",
    "        possible_paths = list(Path(\"/content\").glob(\"**/Data_Entry_2017.csv\"))\n",
    "\n",
    "    if possible_paths:\n",
    "        COLAB_DATA = possible_paths[0].parent\n",
    "        METADATA_PATH = possible_paths[0]\n",
    "        print(f\"Znaleziono zbiór danych w: {COLAB_DATA}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Nie znaleziono zbioru danych! Ponownie uruchom komórkę 2, aby go pobrać.\")\n",
    "\n",
    "# Konfiguracja modelu\n",
    "MODEL_CONFIG = {\n",
    "    \"architecture\": \"DenseNet121\",\n",
    "    \"input_channels\": 1,\n",
    "    \"dropout\": 0.4,\n",
    "    \"spatial_dropout\": 0.2,\n",
    "    \"image_size\": 512,\n",
    "}\n",
    "\n",
    "# Konfiguracja trenowania\n",
    "TRAIN_CONFIG = {\n",
    "    \"stage1_epochs\": 5,\n",
    "    \"stage2_epochs\": 10,\n",
    "    \"stage3_epochs\": 15,\n",
    "    \"batch_size_s1_s2\": 32,\n",
    "    \"batch_size_s3\": 16,\n",
    "    \"num_workers\": 2,\n",
    "    \"base_lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"early_stopping_patience\": 3,\n",
    "    \"grad_clip_max_norm\": 1.0,\n",
    "    \"use_weighted_sampler\": True,\n",
    "}\n",
    "\n",
    "print(f\"Katalog z danymi: {COLAB_DATA}\")\n",
    "print(f\"Katalog roboczy: {COLAB_WORKING}\")\n",
    "print(f\"Istnienie metadanych: {METADATA_PATH.exists()}\")\n",
    "\n",
    "# Wypisz katalogi z obrazami\n",
    "image_dirs = sorted([d for d in COLAB_DATA.iterdir() if d.is_dir() and d.name.startswith(\"images_\")])\n",
    "print(f\"Znaleziono {len(image_dirs)} katalogów z obrazami\")\n",
    "\n",
    "# Zapisz konfigurację\n",
    "with open(COLAB_WORKING / \"logs/config.json\", \"w\") as f:\n",
    "    json.dump({\"model\": MODEL_CONFIG, \"training\": TRAIN_CONFIG}, f, indent=2)\n",
    "print(\"Konfiguracja została zapisana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd88653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 6: Klasa zbioru danych\n",
    "\n",
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, dataframe, data_dir, transform=None, target_class='Mass'):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.transform = transform\n",
    "        self.target_class = target_class\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Pobranie ścieżki do obrazu\n",
    "        image_name = self.dataframe.iloc[idx]['Image Index']\n",
    "\n",
    "        # Znalezienie obrazu w podkatalogach\n",
    "        image_path = None\n",
    "        for i in range(1, 13):\n",
    "            potential_path = self.data_dir / f\"images_{i:03d}\" / \"images\" / image_name\n",
    "            if potential_path.exists():\n",
    "                image_path = potential_path\n",
    "                break\n",
    "\n",
    "        if image_path is None:\n",
    "            raise FileNotFoundError(f\"Nie znaleziono obrazu: {image_name}\")\n",
    "\n",
    "        # Wczytanie obrazu w skali szarości\n",
    "        image = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Nie udało się wczytać obrazu: {image_path}\")\n",
    "\n",
    "        # Zastosowanie CLAHE z adaptacyjnymi parametrami\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        image = clahe.apply(image)\n",
    "\n",
    "        # Zastosowanie transformacji\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "\n",
    "        # Etykieta binarna\n",
    "        finding_labels = self.dataframe.iloc[idx]['Finding Labels']\n",
    "        target = 1.0 if self.target_class in finding_labels else 0.0\n",
    "        target = torch.tensor([target], dtype=torch.float32)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "print(\"Zdefiniowano klasę zbioru danych\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c29b3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 7: Transformacje danych\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    # Skalowanie dłuższego boku do docelowego rozmiaru (z zachowaniem proporcji)\n",
    "    A.LongestMaxSize(max_size=MODEL_CONFIG[\"image_size\"]),\n",
    "    # Uzupełnienie do kwadratu czarnymi ramkami (lepsze niż rozciąganie obrazu)\n",
    "    A.PadIfNeeded(\n",
    "        min_height=MODEL_CONFIG[\"image_size\"],\n",
    "        min_width=MODEL_CONFIG[\"image_size\"],\n",
    "        border_mode=cv2.BORDER_CONSTANT,\n",
    "        value=0,\n",
    "        p=1.0,\n",
    "    ),\n",
    "    # Augmentacje\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=10, p=0.5),\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),\n",
    "    A.GaussNoise(var_limit=(10, 50), p=0.3),\n",
    "    # CoarseDropout, aby ograniczyć przeuczanie się na rogach obrazu\n",
    "    A.CoarseDropout(\n",
    "        max_holes=8,\n",
    "        max_height=32,\n",
    "        max_width=32,\n",
    "        min_holes=2,\n",
    "        fill_value=0,\n",
    "        p=0.3,\n",
    "    ),\n",
    "    # Odpowiednia normalizacja dla obrazów w skali szarości\n",
    "    A.Normalize(mean=[0.5], std=[0.5]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    # Taka sama wstępna obróbka jak w treningu (bez augmentacji)\n",
    "    A.LongestMaxSize(max_size=MODEL_CONFIG[\"image_size\"]),\n",
    "    A.PadIfNeeded(\n",
    "        min_height=MODEL_CONFIG[\"image_size\"],\n",
    "        min_width=MODEL_CONFIG[\"image_size\"],\n",
    "        border_mode=cv2.BORDER_CONSTANT,\n",
    "        value=0,\n",
    "        p=1.0,\n",
    "    ),\n",
    "    A.Normalize(mean=[0.5], std=[0.5]),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "print(\"Zdefiniowano transformacje (z zachowaniem proporcji i poprawną normalizacją)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ad55b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 8: Tworzenie podziałów z uwzględnieniem pacjentów\n",
    "\n",
    "# Sprawdź, czy podziały już istnieją na Dysku Google\n",
    "train_split_path = COLAB_WORKING / \"splits/train_split.csv\"\n",
    "val_split_path = COLAB_WORKING / \"splits/val_split.csv\"\n",
    "test_split_path = COLAB_WORKING / \"splits/test_split.csv\"\n",
    "\n",
    "if train_split_path.exists() and val_split_path.exists() and test_split_path.exists():\n",
    "    print(\"Podziały już istnieją na Dysku, wczytuję z plików...\")\n",
    "    train_df = pd.read_csv(train_split_path)\n",
    "    val_df = pd.read_csv(val_split_path)\n",
    "    test_df = pd.read_csv(test_split_path)\n",
    "else:\n",
    "    print(\"Tworzenie podziałów z uwzględnieniem pacjentów...\")\n",
    "\n",
    "    # Wczytanie metadanych\n",
    "    metadata_df = pd.read_csv(METADATA_PATH)\n",
    "    print(f\"  Liczba obrazów: {len(metadata_df):,}\")\n",
    "\n",
    "    # Wyodrębnienie identyfikatorów pacjentów\n",
    "    metadata_df['Patient ID'] = metadata_df['Image Index'].str.split('_').str[0]\n",
    "    metadata_df['Mass'] = metadata_df['Finding Labels'].str.contains('Mass').astype(int)\n",
    "\n",
    "    unique_patients = metadata_df['Patient ID'].nunique()\n",
    "    print(f\"  Liczba unikalnych pacjentów: {unique_patients:,}\")\n",
    "\n",
    "    # Podział na poziomie pacjenta\n",
    "    gss_test = GroupShuffleSplit(n_splits=1, test_size=0.15, random_state=42)\n",
    "    train_val_idx, test_idx = next(gss_test.split(\n",
    "        metadata_df,\n",
    "        groups=metadata_df['Patient ID']\n",
    "    ))\n",
    "\n",
    "    train_val_df = metadata_df.iloc[train_val_idx].reset_index(drop=True)\n",
    "    test_df = metadata_df.iloc[test_idx].reset_index(drop=True)\n",
    "\n",
    "    gss_val = GroupShuffleSplit(n_splits=1, test_size=0.176, random_state=42)\n",
    "    train_idx, val_idx = next(gss_val.split(\n",
    "        train_val_df,\n",
    "        groups=train_val_df['Patient ID']\n",
    "    ))\n",
    "\n",
    "    train_df = train_val_df.iloc[train_idx].reset_index(drop=True)\n",
    "    val_df = train_val_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    # Zapisanie podziałów na Dysku Google\n",
    "    train_df.to_csv(train_split_path, index=False)\n",
    "    val_df.to_csv(val_split_path, index=False)\n",
    "    test_df.to_csv(test_split_path, index=False)\n",
    "\n",
    "    print(\"  Zapisano podziały na Dysku Google\")\n",
    "\n",
    "# Statystyki podziału\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"PODZIAŁY Z UWZGLĘDNIENIEM PACJENTÓW\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Zbiór treningowy: {len(train_df):,} obrazów, {train_df['Patient ID'].nunique():,} pacjentów\")\n",
    "print(f\"Zbiór walidacyjny: {len(val_df):,} obrazów, {val_df['Patient ID'].nunique():,} pacjentów\")\n",
    "print(f\"Zbiór testowy:    {len(test_df):,} obrazów, {test_df['Patient ID'].nunique():,} pacjentów\")\n",
    "\n",
    "# Sprawdzenie braku nakładania się pacjentów między zbiorami\n",
    "train_patients = set(train_df['Patient ID'])\n",
    "val_patients = set(val_df['Patient ID'])\n",
    "test_patients = set(test_df['Patient ID'])\n",
    "\n",
    "overlap_train_test = train_patients & test_patients\n",
    "overlap_val_test = val_patients & test_patients\n",
    "overlap_train_val = train_patients & val_patients\n",
    "\n",
    "print(\"\\nWeryfikacja nakładania się pacjentów między zbiorami:\")\n",
    "print(f\"  Train–Test: {len(overlap_train_test)} pacjentów (powinno być 0)\")\n",
    "print(f\"  Val–Test:   {len(overlap_val_test)} pacjentów (powinno być 0)\")\n",
    "print(f\"  Train–Val:  {len(overlap_train_val)} pacjentów (powinno być 0)\")\n",
    "\n",
    "if overlap_train_test or overlap_val_test or overlap_train_val:\n",
    "    raise RuntimeError(\"Wykryto przeciek danych między zbiorami!\")\n",
    "\n",
    "print(\"\\nPodziały danych zostały poprawnie wczytane\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f2ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 9: Tworzenie DataLoaderów (Etapy 1 i 2)\n",
    "\n",
    "\n",
    "batch_size_s1_s2 = TRAIN_CONFIG[\"batch_size_s1_s2\"]\n",
    "num_workers = TRAIN_CONFIG[\"num_workers\"]\n",
    "\n",
    "# Utworzenie zbiorów danych\n",
    "train_dataset = ChestXrayDataset(train_df, COLAB_DATA, transform=train_transform)\n",
    "val_dataset = ChestXrayDataset(val_df, COLAB_DATA, transform=val_transform)\n",
    "test_dataset = ChestXrayDataset(test_df, COLAB_DATA, transform=val_transform)\n",
    "\n",
    "# Obliczenie wag klas do zbalansowanego próbkowania\n",
    "train_targets = train_df['Finding Labels'].str.contains('Mass').astype(int).values\n",
    "pos_count = train_targets.sum()\n",
    "neg_count = len(train_targets) - pos_count\n",
    "class_counts = np.array([neg_count, pos_count])\n",
    "class_weights = 1.0 / class_counts\n",
    "sample_weights = class_weights[train_targets]\n",
    "\n",
    "print(f\"\\nRozkład klas w zbiorze treningowym:\")\n",
    "print(f\"  Negatywne (brak zmiany Mass): {neg_count:,} ({neg_count/len(train_targets)*100:.1f}%)\")\n",
    "print(f\"  Pozytywne (Mass):             {pos_count:,} ({pos_count/len(train_targets)*100:.1f}%)\")\n",
    "print(f\"  Stosunek (Neg:Pos):           {neg_count/pos_count:.2f}:1\")\n",
    "\n",
    "# Utworzenie WeightedRandomSampler do zbalansowanego trenowania\n",
    "if TRAIN_CONFIG.get(\"use_weighted_sampler\", True):\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=sample_weights,\n",
    "        num_samples=len(sample_weights),\n",
    "        replacement=True\n",
    "    )\n",
    "    train_loader_s1_s2 = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size_s1_s2,\n",
    "        sampler=sampler,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"\\nUtworzono DataLoadery (Etapy 1 i 2, batch_size={batch_size_s1_s2}):\")\n",
    "    print(\"  Zastosowano WeightedRandomSampler (zbalansowane próbkowanie klas)\")\n",
    "else:\n",
    "    train_loader_s1_s2 = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size_s1_s2,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(f\"\\nUtworzono DataLoadery (Etapy 1 i 2, batch_size={batch_size_s1_s2}):\")\n",
    "    print(\"  WeightedRandomSampler wyłączony (niezbalansowane próbkowanie)\")\n",
    "\n",
    "val_loader_s1_s2 = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size_s1_s2,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"  Liczba batchy w treningu: {len(train_loader_s1_s2)}\")\n",
    "print(f\"  Liczba batchy w walidacji: {len(val_loader_s1_s2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c68f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 10: Definicja modelu\n",
    "\n",
    "class MassDetectionModel(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained=True, dropout=0.4):\n",
    "        super(MassDetectionModel, self).__init__()\n",
    "\n",
    "        self.backbone = models.densenet121(pretrained=pretrained)\n",
    "\n",
    "        # Zmodyfikowanie pierwszej warstwy konwolucyjnej dla obrazu w skali szarości\n",
    "        original_conv = self.backbone.features.conv0\n",
    "        self.backbone.features.conv0 = nn.Conv2d(\n",
    "            1, 64, kernel_size=7, stride=2, padding=3, bias=False\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.backbone.features.conv0.weight = nn.Parameter(\n",
    "                original_conv.weight.mean(dim=1, keepdim=True)\n",
    "            )\n",
    "\n",
    "        num_features = self.backbone.classifier.in_features\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "\n",
    "        # Spatial Dropout2d, aby ograniczyć przeuczanie przestrzenne (artefakty w rogach)\n",
    "        self.spatial_dropout = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(num_features, 1)\n",
    "        )\n",
    "\n",
    "        nn.init.kaiming_normal_(self.head[3].weight, mode='fan_out')\n",
    "        nn.init.zeros_(self.head[3].bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone.features(x)\n",
    "        # Zastosowanie spatial dropout podczas trenowania\n",
    "        if self.training:\n",
    "            features = self.spatial_dropout(features)\n",
    "        output = self.head(features)\n",
    "        return output\n",
    "\n",
    "    def freeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def unfreeze_last_block(self):\n",
    "        for name, param in self.backbone.named_parameters():\n",
    "            if any(block in name for block in ['denseblock4', 'transition3', 'norm5']):\n",
    "                param.requires_grad = True\n",
    "\n",
    "print(\"Zdefiniowano architekturę modelu (ze Spatial Dropout ograniczającym artefakty w rogach)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a570346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 11: Inicjalizacja modelu i funkcji straty\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MassDetectionModel(pretrained=True, dropout=MODEL_CONFIG[\"dropout\"]).to(device)\n",
    "\n",
    "print(f\"Model zainicjalizowany na urządzeniu: {device}\")\n",
    "print(f\"Łączna liczba parametrów: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Liczba parametrów trenowalnych: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Informacja o niezbalansowaniu klas\n",
    "train_targets = train_df['Finding Labels'].str.contains('Mass').astype(int).values\n",
    "pos_count = train_targets.sum()\n",
    "neg_count = len(train_targets) - pos_count\n",
    "imbalance_ratio = neg_count / pos_count\n",
    "\n",
    "print(f\"\\nNiezbalansowanie klas: {imbalance_ratio:.1f}:1 (negatywne:pozytywne)\")\n",
    "print(f\"  Negatywne: {neg_count:,}, Pozytywne: {pos_count:,}\")\n",
    "print(\"\\nFunkcja straty: BCEWithLogitsLoss (standardowa, bez pos_weight)\")\n",
    "print(\"  Równoważenie klas realizowane przez WeightedRandomSampler (lepsze przy dużym niezbalansowaniu)\")\n",
    "print(\"  Unikamy podwójnego równoważenia (pos_weight + WeightedSampler)\")\n",
    "\n",
    "# Standardowa funkcja straty bez pos_weight (równoważenie tylko przez sampler)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81749e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 12: Fabryka optymalizatora\n",
    "\n",
    "def create_optimizer(model, stage, base_lr=1e-3, weight_decay=1e-4):\n",
    "    if stage == 1:\n",
    "        lr_backbone, lr_head = 0.0, base_lr\n",
    "    elif stage == 2:\n",
    "        lr_backbone, lr_head = base_lr / 10, base_lr\n",
    "    elif stage == 3:\n",
    "        lr_backbone, lr_head = base_lr / 100, base_lr\n",
    "    else:\n",
    "        raise ValueError(f\"Niepoprawny etap: {stage}\")\n",
    "\n",
    "    backbone_params = []\n",
    "    head_params = []\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        if 'head' in name:\n",
    "            head_params.append(param)\n",
    "        else:\n",
    "            backbone_params.append(param)\n",
    "\n",
    "    param_groups = [\n",
    "        {'params': backbone_params, 'lr': lr_backbone, 'name': 'backbone'},\n",
    "        {'params': head_params, 'lr': lr_head, 'name': 'head'}\n",
    "    ]\n",
    "\n",
    "    optimizer = optim.AdamW(param_groups, betas=(0.9, 0.999), eps=1e-8, weight_decay=weight_decay)\n",
    "    return optimizer\n",
    "\n",
    "print(\"Utworzono fabrykę optymalizatora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07297c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 13: Funkcje trenowania i walidacji\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, stage_name=\"\"):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"{stage_name} Epoka {epoch} - trening\")\n",
    "    for images, targets in pbar:\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=TRAIN_CONFIG[\"grad_clip_max_norm\"])\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        all_preds.extend(probs.squeeze())\n",
    "        all_targets.extend(targets.cpu().numpy().squeeze())\n",
    "\n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    auc = roc_auc_score(all_targets, all_preds)\n",
    "\n",
    "    return avg_loss, auc\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device, desc=\"Walidacja\"):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(val_loader, desc=desc):\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            all_preds.extend(probs.squeeze())\n",
    "            all_targets.extend(targets.cpu().numpy().squeeze())\n",
    "\n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    auc = roc_auc_score(all_targets, all_preds)\n",
    "\n",
    "    return avg_loss, auc\n",
    "\n",
    "print(\"Zdefiniowano funkcje trenowania\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7297cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 14: Klasa wczesnego zatrzymania (Early Stopping)\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_auc = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_auc):\n",
    "        if self.best_auc is None:\n",
    "            self.best_auc = val_auc\n",
    "        elif val_auc < self.best_auc + self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_auc = val_auc\n",
    "            self.counter = 0\n",
    "\n",
    "        return self.early_stop\n",
    "\n",
    "print(\"Zdefiniowano mechanizm wczesnego zatrzymania\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5a1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 15: ETAP 1 – ekstrakcja cech\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ETAP 1: EKSTRAKCJA CECH\")\n",
    "print(\"=\"*80)\n",
    "print(\"Strategia: zamrożony backbone, trenowana tylko głowa klasyfikacyjna\")\n",
    "print(f\"Liczba epok: {TRAIN_CONFIG['stage1_epochs']}, rozmiar batcha: {TRAIN_CONFIG['batch_size_s1_s2']}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "model.freeze_backbone()\n",
    "optimizer = create_optimizer(model, stage=1, base_lr=TRAIN_CONFIG[\"base_lr\"])\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, min_lr=1e-7)\n",
    "early_stopping = EarlyStopping(patience=TRAIN_CONFIG[\"early_stopping_patience\"])\n",
    "\n",
    "best_auc_s1 = 0.0\n",
    "history_s1 = []\n",
    "start_time_s1 = time.time()\n",
    "\n",
    "for epoch in range(1, TRAIN_CONFIG['stage1_epochs'] + 1):\n",
    "    train_loss, train_auc = train_one_epoch(\n",
    "        model, train_loader_s1_s2, criterion, optimizer, device, epoch, \"Etap 1\"\n",
    ")\n",
    "    val_loss, val_auc = validate(model, val_loader_s1_s2, criterion, device)\n",
    "    scheduler.step(val_auc)\n",
    "\n",
    "    print(f\"\\nEpoka {epoch}/{TRAIN_CONFIG['stage1_epochs']}:\")\n",
    "    print(f\"  Trening - Loss: {train_loss:.4f}, AUC: {train_auc:.4f}\")\n",
    "    print(f\"  Walidacja - Loss: {val_loss:.4f}, AUC: {val_auc:.4f}\")\n",
    "\n",
    "    history_s1.append({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss,\n",
    "        'train_auc': train_auc,\n",
    "        'val_loss': val_loss,\n",
    "        'val_auc': val_auc,\n",
    "    })\n",
    "\n",
    "    if val_auc > best_auc_s1:\n",
    "        best_auc_s1 = val_auc\n",
    "        torch.save(model.state_dict(), COLAB_WORKING / \"models/best_model_stage1.pth\")\n",
    "        print(f\"  Zapisano najlepszy model na Dysku (AUC: {best_auc_s1:.4f})\")\n",
    "\n",
    "    if early_stopping(val_auc):\n",
    "        print(f\"\\nWczesne zatrzymanie trenowania w epoce {epoch}\")\n",
    "        break\n",
    "\n",
    "elapsed_s1 = time.time() - start_time_s1\n",
    "print(f\"\\nEtap 1 zakończony – najlepsze AUC: {best_auc_s1:.4f}, czas: {elapsed_s1/60:.1f} min\")\n",
    "\n",
    "with open(COLAB_WORKING / \"logs/history_stage1.json\", \"w\") as f:\n",
    "    json.dump(history_s1, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d073fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 16: ETAP 2 – częściowe dostrajanie\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ETAP 2: CZĘŚCIOWE DOSTRAJANIE\")\n",
    "print(\"=\"*80)\n",
    "print(\"Strategia: odmrożenie ostatniego bloku sieci\")\n",
    "print(f\"Liczba epok: {TRAIN_CONFIG['stage2_epochs']}, rozmiar batcha: {TRAIN_CONFIG['batch_size_s1_s2']}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "model.load_state_dict(torch.load(COLAB_WORKING / \"models/best_model_stage1.pth\"))\n",
    "model.unfreeze_last_block()\n",
    "\n",
    "optimizer = create_optimizer(model, stage=2, base_lr=TRAIN_CONFIG[\"base_lr\"])\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, min_lr=1e-7)\n",
    "early_stopping = EarlyStopping(patience=TRAIN_CONFIG[\"early_stopping_patience\"])\n",
    "\n",
    "best_auc_s2 = 0.0\n",
    "history_s2 = []\n",
    "start_time_s2 = time.time()\n",
    "\n",
    "for epoch in range(1, TRAIN_CONFIG['stage2_epochs'] + 1):\n",
    "    train_loss, train_auc = train_one_epoch(\n",
    "        model, train_loader_s1_s2, criterion, optimizer, device, epoch, \"Etap 2\"\n",
    ")\n",
    "    val_loss, val_auc = validate(model, val_loader_s1_s2, criterion, device)\n",
    "    scheduler.step(val_auc)\n",
    "\n",
    "    print(f\"\\nEpoka {epoch}/{TRAIN_CONFIG['stage2_epochs']}:\")\n",
    "    print(f\"  Trening - Loss: {train_loss:.4f}, AUC: {train_auc:.4f}\")\n",
    "    print(f\"  Walidacja - Loss: {val_loss:.4f}, AUC: {val_auc:.4f}\")\n",
    "\n",
    "    history_s2.append({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss,\n",
    "        'train_auc': train_auc,\n",
    "        'val_loss': val_loss,\n",
    "        'val_auc': val_auc,\n",
    "    })\n",
    "\n",
    "    if val_auc > best_auc_s2:\n",
    "        best_auc_s2 = val_auc\n",
    "        torch.save(model.state_dict(), COLAB_WORKING / \"models/best_model_stage2.pth\")\n",
    "        print(f\"  Zapisano najlepszy model na Dysku (AUC: {best_auc_s2:.4f})\")\n",
    "\n",
    "    if early_stopping(val_auc):\n",
    "        print(f\"\\nWczesne zatrzymanie trenowania w epoce {epoch}\")\n",
    "        break\n",
    "\n",
    "elapsed_s2 = time.time() - start_time_s2\n",
    "print(f\"\\nEtap 2 zakończony – najlepsze AUC: {best_auc_s2:.4f}, czas: {elapsed_s2/60:.1f} min\")\n",
    "\n",
    "with open(COLAB_WORKING / \"logs/history_stage2.json\", \"w\") as f:\n",
    "    json.dump(history_s2, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854df58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 17: DataLoadery dla Etapu 3\n",
    "\n",
    "batch_size_s3 = TRAIN_CONFIG[\"batch_size_s3\"]\n",
    "\n",
    "# Użycie tego samego WeightedRandomSampler dla Etapu 3\n",
    "if TRAIN_CONFIG.get(\"use_weighted_sampler\", True):\n",
    "    train_loader_s3 = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size_s3,\n",
    "        sampler=sampler,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(\"Etap 3 wykorzystuje WeightedRandomSampler (zbalansowane próbkowanie)\")\n",
    "else:\n",
    "    train_loader_s3 = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size_s3,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "val_loader_s3 = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size_s3,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Utworzono DataLoadery dla Etapu 3 (batch_size={batch_size_s3}):\")\n",
    "print(f\"  Liczba batchy w treningu: {len(train_loader_s3)}\")\n",
    "print(f\"  Liczba batchy w walidacji: {len(val_loader_s3)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946efcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 18: ETAP 3 – pełne dostrajanie\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ETAP 3: PEŁNE DOSTRAJANIE\")\n",
    "print(\"=\"*80)\n",
    "print(\"Strategia: odmrożenie całego backbone\")\n",
    "print(f\"Liczba epok: {TRAIN_CONFIG['stage3_epochs']}, rozmiar batcha: {TRAIN_CONFIG['batch_size_s3']}\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "model.load_state_dict(torch.load(COLAB_WORKING / \"models/best_model_stage2.pth\"))\n",
    "model.unfreeze_backbone()\n",
    "\n",
    "optimizer = create_optimizer(model, stage=3, base_lr=TRAIN_CONFIG[\"base_lr\"])\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, min_lr=1e-7)\n",
    "early_stopping = EarlyStopping(patience=TRAIN_CONFIG[\"early_stopping_patience\"])\n",
    "\n",
    "best_auc_s3 = 0.0\n",
    "history_s3 = []\n",
    "start_time_s3 = time.time()\n",
    "\n",
    "for epoch in range(1, TRAIN_CONFIG['stage3_epochs'] + 1):\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    train_loss, train_auc = train_one_epoch(\n",
    "        model, train_loader_s3, criterion, optimizer, device, epoch, \"Etap 3\"\n",
    ")\n",
    "    val_loss, val_auc = validate(model, val_loader_s3, criterion, device)\n",
    "    scheduler.step(val_auc)\n",
    "\n",
    "    print(f\"\\nEpoka {epoch}/{TRAIN_CONFIG['stage3_epochs']}:\")\n",
    "    print(f\"  Trening - Loss: {train_loss:.4f}, AUC: {train_auc:.4f}\")\n",
    "    print(f\"  Walidacja - Loss: {val_loss:.4f}, AUC: {val_auc:.4f}\")\n",
    "\n",
    "    history_s3.append({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss,\n",
    "        'train_auc': train_auc,\n",
    "        'val_loss': val_loss,\n",
    "        'val_auc': val_auc,\n",
    "    })\n",
    "\n",
    "    if val_auc > best_auc_s3:\n",
    "        best_auc_s3 = val_auc\n",
    "        torch.save(model.state_dict(), COLAB_WORKING / \"models/best_model_stage3.pth\")\n",
    "        print(f\"  Zapisano najlepszy model na Dysku (AUC: {best_auc_s3:.4f})\")\n",
    "\n",
    "    if early_stopping(val_auc):\n",
    "        print(f\"\\nWczesne zatrzymanie trenowania w epoce {epoch}\")\n",
    "        break\n",
    "\n",
    "elapsed_s3 = time.time() - start_time_s3\n",
    "print(f\"\\nEtap 3 zakończony – najlepsze AUC: {best_auc_s3:.4f}, czas: {elapsed_s3/60:.1f} min\")\n",
    "\n",
    "with open(COLAB_WORKING / \"logs/history_stage3.json\", \"w\") as f:\n",
    "    json.dump(history_s3, f, indent=2)\n",
    "\n",
    "history_combined = {\n",
    "    'stage1': history_s1,\n",
    "    'stage2': history_s2,\n",
    "    'stage3': history_s3,\n",
    "    'summary': {\n",
    "        'best_auc_stage1': best_auc_s1,\n",
    "        'best_auc_stage2': best_auc_s2,\n",
    "        'best_auc_stage3': best_auc_s3,\n",
    "        'total_time_minutes': (elapsed_s1 + elapsed_s2 + elapsed_s3) / 60,\n",
    "    }\n",
    "}\n",
    "with open(COLAB_WORKING / \"logs/training_history_complete.json\", \"w\") as f:\n",
    "    json.dump(history_combined, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRENOWANIE ZAKOŃCZONE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Etap 1 – najlepsze AUC: {best_auc_s1:.4f} (czas: {elapsed_s1/60:.1f} min)\")\n",
    "print(f\"Etap 2 – najlepsze AUC: {best_auc_s2:.4f} (czas: {elapsed_s2/60:.1f} min)\")\n",
    "print(f\"Etap 3 – najlepsze AUC: {best_auc_s3:.4f} (czas: {elapsed_s3/60:.1f} min)\")\n",
    "print(f\"Łączny czas: {(elapsed_s1 + elapsed_s2 + elapsed_s3)/60:.1f} min\")\n",
    "print(f\"\\nWszystkie checkpointy zapisano na Dysku Google: {COLAB_WORKING}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9962d83",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# EVALUATION PIPELINE\n",
    "\n",
    "Comprehensive test set evaluation with metrics, visualizations, and Grad-CAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fcbaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 19: Ewaluacja na zbiorze testowym\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EWALUACJA NA ZBIORZE TESTOWYM\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(COLAB_WORKING / \"models/best_model_stage3.pth\"))\n",
    "print(f\"Wczytano najlepszy model z Etapu 3 (AUC walidacyjne: {best_auc_s3:.4f})\")\n",
    "\n",
    "start_time_eval = time.time()\n",
    "model.eval()\n",
    "all_targets = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, targets in tqdm(test_loader, desc=\"Ewaluacja na zbiorze testowym\"):\n",
    "        images = images.to(device)\n",
    "        targets = targets.to(device)\n",
    "        logits = model(images)\n",
    "        probs = torch.sigmoid(logits).cpu().numpy().squeeze()\n",
    "        all_probs.extend(probs)\n",
    "        all_targets.extend(targets.cpu().numpy().squeeze())\n",
    "\n",
    "all_targets = np.array(all_targets)\n",
    "all_probs = np.array(all_probs)\n",
    "\n",
    "eval_time = time.time() - start_time_eval\n",
    "print(f\"Ewaluacja zakończona (czas: {eval_time/60:.1f} min)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5020a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 20: Podstawowe metryki\n",
    "\n",
    "threshold_default = 0.5\n",
    "preds_default = (all_probs >= threshold_default).astype(int)\n",
    "\n",
    "auc_test = roc_auc_score(all_targets, all_probs)\n",
    "acc_test = accuracy_score(all_targets, preds_default)\n",
    "cm_default = confusion_matrix(all_targets, preds_default)\n",
    "report_default = classification_report(all_targets, preds_default,\n",
    "                                       target_names=[\"No Finding\", \"Mass\"], digits=4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"WYNIKI NA ZBIORZE TESTOWYM (Próg={threshold_default})\")\n",
    "print(\"=\"*80)\n",
    "print(f\"AUC-ROC: {auc_test:.4f}\")\n",
    "print(f\"Accuracy: {acc_test:.4f}\")\n",
    "print(\"\\nMacierz pomyłek:\")\n",
    "print(cm_default)\n",
    "print(\"\\nRaport klasyfikacji:\")\n",
    "print(report_default)\n",
    "\n",
    "tn, fp, fn, tp = cm_default.ravel()\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "ppv = tp / (tp + fp)\n",
    "npv = tn / (tn + fn)\n",
    "\n",
    "print(\"\\nMetryki kliniczne:\")\n",
    "print(f\"  Czułość (Sensitivity): {sensitivity:.4f}\")\n",
    "print(f\"  Swoistość (Specificity): {specificity:.4f}\")\n",
    "print(f\"  PPV: {ppv:.4f}\")\n",
    "print(f\"  NPV: {npv:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ad67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 21: Optymalizacja progu\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OPTYMALIZACJA PROGU DECYZYJNEGO\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "precisions, recalls, thresholds_pr = precision_recall_curve(all_targets, all_probs)\n",
    "\n",
    "target_recall = 0.70\n",
    "idx_recall = np.where(recalls >= target_recall)[0]\n",
    "if len(idx_recall) > 0:\n",
    "    optimal_threshold_recall = thresholds_pr[idx_recall[-1]]\n",
    "    optimal_precision_recall = precisions[idx_recall[-1]]\n",
    "else:\n",
    "    optimal_threshold_recall = 0.5\n",
    "    optimal_precision_recall = 0.0\n",
    "\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
    "idx_f1 = np.argmax(f1_scores)\n",
    "optimal_threshold_f1 = thresholds_pr[idx_f1]\n",
    "\n",
    "fpr_roc, tpr_roc, thresholds_roc = roc_curve(all_targets, all_probs)\n",
    "youdens = tpr_roc - fpr_roc\n",
    "idx_youden = np.argmax(youdens)\n",
    "optimal_threshold_youden = thresholds_roc[idx_youden]\n",
    "\n",
    "print(\"Optymalne progi:\")\n",
    "print(f\"  1. Czułość ≥ 70%:  {optimal_threshold_recall:.4f} (Precyzja: {optimal_precision_recall:.4f})\")\n",
    "print(f\"  2. Maksymalny F1-score:  {optimal_threshold_f1:.4f} (F1: {f1_scores[idx_f1]:.4f})\")\n",
    "print(f\"  3. Wskaźnik Youdena: {optimal_threshold_youden:.4f}\")\n",
    "\n",
    "preds_optimal = (all_probs >= optimal_threshold_recall).astype(int)\n",
    "cm_optimal = confusion_matrix(all_targets, preds_optimal)\n",
    "report_optimal = classification_report(all_targets, preds_optimal,\n",
    "                                      target_names=[\"No Finding\", \"Mass\"], digits=4)\n",
    "\n",
    "print(f\"\\nWyniki dla optymalnego progu ({optimal_threshold_recall:.4f}):\")\n",
    "print(\"\\nMacierz pomyłek:\")\n",
    "print(cm_optimal)\n",
    "print(\"\\nRaport klasyfikacji:\")\n",
    "print(report_optimal)\n",
    "\n",
    "threshold_data = {\n",
    "    \"default_threshold\": float(threshold_default),\n",
    "    \"optimal_threshold_recall_70\": float(optimal_threshold_recall),\n",
    "    \"optimal_threshold_f1\": float(optimal_threshold_f1),\n",
    "    \"optimal_threshold_youden\": float(optimal_threshold_youden),\n",
    "}\n",
    "\n",
    "with open(COLAB_WORKING / \"logs/threshold_optimization.json\", \"w\") as f:\n",
    "    json.dump(threshold_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7b116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 22: Zapis metryk i predykcji\n",
    "\n",
    "test_metrics = {\n",
    "    \"auc_roc\": float(auc_test),\n",
    "    \"accuracy\": float(acc_test),\n",
    "    \"clinical_metrics\": {\n",
    "        \"sensitivity\": float(sensitivity),\n",
    "        \"specificity\": float(specificity),\n",
    "        \"ppv\": float(ppv),\n",
    "        \"npv\": float(npv)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(COLAB_WORKING / \"logs/test_metrics.json\", \"w\") as f:\n",
    "    json.dump(test_metrics, f, indent=2)\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    'image_index': test_df['Image Index'].values,\n",
    "    'patient_id': test_df['Patient ID'].values,\n",
    "    'true_label': all_targets,\n",
    "    'predicted_prob': all_probs,\n",
    "    'predicted_class_threshold_05': preds_default,\n",
    "    'predicted_class_optimal': preds_optimal\n",
    "})\n",
    "predictions_df.to_csv(COLAB_WORKING / \"logs/all_predictions.csv\", index=False)\n",
    "\n",
    "print(\"Metryki zapisano na Dysku Google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843aa0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 23: Krzywa ROC\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(fpr_roc, tpr_roc, linewidth=2, label=f'Krzywa ROC (AUC = {auc_test:.4f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Losowy klasyfikator')\n",
    "plt.scatter([fpr_roc[idx_youden]], [tpr_roc[idx_youden]], color='red', s=100,\n",
    "            zorder=5, label=f\"Punkt Youdena (próg={optimal_threshold_youden:.3f})\")\n",
    "plt.xlabel('Odsetek fałszywie pozytywnych (FPR)', fontsize=12)\n",
    "plt.ylabel('Odsetek prawdziwie pozytywnych (TPR)', fontsize=12)\n",
    "plt.title('Krzywa ROC – detekcja zmian typu Mass', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(COLAB_WORKING / \"logs/roc_curve.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Krzywą ROC zapisano na Dysku Google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268638b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 24: Krzywa precyzja–czułość\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(recalls, precisions, linewidth=2, label='Krzywa precyzja–czułość')\n",
    "plt.scatter([target_recall], [optimal_precision_recall], color='red', s=100,\n",
    "            zorder=5, label=f'Czułość ≥ 70% (próg={optimal_threshold_recall:.3f})')\n",
    "plt.axhline(y=optimal_precision_recall, color='red', linestyle='--', alpha=0.3)\n",
    "plt.axvline(x=target_recall, color='red', linestyle='--', alpha=0.3)\n",
    "plt.xlabel('Czułość (Recall)', fontsize=12)\n",
    "plt.ylabel('Precyzja (Precision)', fontsize=12)\n",
    "plt.title('Krzywa precyzja–czułość', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(COLAB_WORKING / \"logs/precision_recall_curve.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Krzywą precyzja–czułość zapisano na Dysku Google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4c170f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 25: Macierze pomyłek\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "sns.heatmap(cm_default, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[\"No Finding\", \"Mass\"],\n",
    "            yticklabels=[\"No Finding\", \"Mass\"],\n",
    "            ax=axes[0], cbar=True, annot_kws={\"fontsize\": 12})\n",
    "axes[0].set_xlabel('Klasa przewidziana', fontsize=11)\n",
    "axes[0].set_ylabel('Klasa rzeczywista', fontsize=11)\n",
    "axes[0].set_title(f'Próg={threshold_default}', fontsize=12, fontweight='bold')\n",
    "\n",
    "sns.heatmap(cm_optimal, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=[\"No Finding\", \"Mass\"],\n",
    "            yticklabels=[\"No Finding\", \"Mass\"],\n",
    "            ax=axes[1], cbar=True, annot_kws={\"fontsize\": 12})\n",
    "axes[1].set_xlabel('Klasa przewidziana', fontsize=11)\n",
    "axes[1].set_ylabel('Klasa rzeczywista', fontsize=11)\n",
    "axes[1].set_title(f'Próg={optimal_threshold_recall:.3f}', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(COLAB_WORKING / \"logs/confusion_matrices.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Macierze pomyłek zapisano na Dysku Google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72c8d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 26: Wizualizacja Grad-CAM\n",
    "\n",
    "def generate_gradcam(model, image_tensor, device='cuda'):\n",
    "    model.eval()\n",
    "    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
    "    image_tensor.requires_grad = True\n",
    "\n",
    "    features = None\n",
    "    gradients = None\n",
    "\n",
    "    def forward_hook(module, input, output):\n",
    "        nonlocal features\n",
    "        features = output\n",
    "\n",
    "    def backward_hook(module, grad_input, grad_output):\n",
    "        nonlocal gradients\n",
    "        gradients = grad_output[0]\n",
    "\n",
    "    target_layer = model.backbone.features.denseblock4\n",
    "    forward_handle = target_layer.register_forward_hook(forward_hook)\n",
    "    backward_handle = target_layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    output = model(image_tensor)\n",
    "\n",
    "    model.zero_grad()\n",
    "    class_loss = output[0, 0]\n",
    "    class_loss.backward()\n",
    "\n",
    "    forward_handle.remove()\n",
    "    backward_handle.remove()\n",
    "\n",
    "    pooled_grads = torch.mean(gradients, dim=[0, 2, 3])\n",
    "    for i in range(features.shape[1]):\n",
    "        features[:, i, :, :] *= pooled_grads[i]\n",
    "\n",
    "    heatmap = features.mean(dim=1).squeeze().cpu().detach().numpy()\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= (np.max(heatmap) + 1e-8)\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "\n",
    "print(\"Wizualizacja Grad-CAM\")\n",
    "images_sample, targets_sample = next(iter(test_loader))\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(min(6, len(images_sample))):\n",
    "    heatmap = generate_gradcam(model, images_sample[i], device=device)\n",
    "    heatmap_resized = cv2.resize(heatmap, (MODEL_CONFIG[\"image_size\"], MODEL_CONFIG[\"image_size\"]))\n",
    "\n",
    "    img = images_sample[i].cpu().squeeze().numpy()\n",
    "    axes[i].imshow(img, cmap='gray')\n",
    "    axes[i].imshow(heatmap_resized, cmap='jet', alpha=0.4)\n",
    "    axes[i].set_title(f\"Etykieta: {'Mass' if targets_sample[i].item() == 1 else 'No Finding'}\", fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Grad-CAM – obszary uwagi modelu', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(COLAB_WORKING / \"logs/gradcam_examples.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Wizualizacje Grad-CAM zapisano na Dysku Google\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ae7ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KOMÓRKA 27: Podsumowanie końcowe\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KOMPLETNY PIPELINE ZAKOŃCZONY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = {\n",
    "    \"training\": {\n",
    "        \"stage1_best_val_auc\": best_auc_s1,\n",
    "        \"stage2_best_val_auc\": best_auc_s2,\n",
    "        \"stage3_best_val_auc\": best_auc_s3,\n",
    "        \"total_time_minutes\": (elapsed_s1 + elapsed_s2 + elapsed_s3) / 60\n",
    "    },\n",
    "    \"evaluation\": {\n",
    "        \"test_auc\": auc_test,\n",
    "        \"test_accuracy\": acc_test,\n",
    "        \"threshold_optimal\": optimal_threshold_recall\n",
    "    },\n",
    "    \"splits\": {\n",
    "        \"train_images\": len(train_df),\n",
    "        \"val_images\": len(val_df),\n",
    "        \"test_images\": len(test_df)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(COLAB_WORKING / \"logs/final_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\nPODSUMOWANIE TRENOWANIA:\")\n",
    "print(f\"  Etap 1: {best_auc_s1:.4f} ({elapsed_s1/60:.1f} min)\")\n",
    "print(f\"  Etap 2: {best_auc_s2:.4f} ({elapsed_s2/60:.1f} min)\")\n",
    "print(f\"  Etap 3: {best_auc_s3:.4f} ({elapsed_s3/60:.1f} min)\")\n",
    "print(f\"  Łącznie: {(elapsed_s1 + elapsed_s2 + elapsed_s3)/60:.1f} min\")\n",
    "\n",
    "print(\"\\nPODSUMOWANIE EWALUACJI TESTOWEJ:\")\n",
    "print(f\"  AUC na zbiorze testowym: {auc_test:.4f}\")\n",
    "print(f\"  Accuracy na zbiorze testowym: {acc_test:.4f}\")\n",
    "\n",
    "print(f\"\\nWszystkie wyniki zapisano na Dysku Google: {COLAB_WORKING}\")\n",
    "print(\"\\nPLIKI NA DYSKU:\")\n",
    "print(\"/MyDrive/mass_detection_training/models/\")\n",
    "print(\"- best_model_stage1.pth\")\n",
    "print(\"- best_model_stage2.pth\")\n",
    "print(\"- best_model_stage3.pth\")\n",
    "print(\"/MyDrive/mass_detection_training/logs/\")\n",
    "print(\"- training_history_complete.json\")\n",
    "print(\"- test_metrics.json\")\n",
    "print(\"- all_predictions.csv\")\n",
    "print(\"- pliki PNG (wizualizacje)\")\n",
    "print(\"/MyDrive/mass_detection_training/splits/\")\n",
    "print(\"- train_split.csv, val_split.csv, test_split.csv\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afae50e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instructions for Google Colab\n",
    "\n",
    "### First Time Setup:\n",
    "1. Change Runtime: Runtime → Change runtime type → T4 GPU\n",
    "2. Run Cell 1: Mount Google Drive (authorize access)\n",
    "3. Run Cell 2: Download dataset (20–40 min, ~45 GB)\n",
    "4. Run Cells 3–27: Complete training pipeline\n",
    "\n",
    "### Resume Training (if session times out):\n",
    "1. Run Cell 1 (mount Drive)\n",
    "2. Skip Cell 2 (dataset already downloaded to `/content/data/`)\n",
    "3. Run Cell 3–8 (setup + load existing splits from Drive)\n",
    "4. Load checkpoint from Drive:\n",
    "   ```python\n",
    "   model.load_state_dict(torch.load(COLAB_WORKING / \"models/best_model_stage2.pth\"))\n",
    "   ```\n",
    "5. Continue from desired stage\n",
    "\n",
    "### Key Features:\n",
    "- Persistent Storage: Checkpoints saved to Google Drive (survives session timeouts)\n",
    "- No Manual Upload: Dataset downloads automatically via Kaggle API\n",
    "- Same Splits: Patient-stratified splits saved to Drive (deterministic)\n",
    "- Resume Capability: Can restart from any stage using Drive checkpoints\n",
    "\n",
    "### Tips:\n",
    "- Colab free tier: ~12h runtime limit (enough for full training)\n",
    "- Dataset stays in `/content/data/` for current session\n",
    "- Checkpoints in Drive persist over time\n",
    "- Re-run Cell 2 only if `/content/data/` is deleted (new session)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
